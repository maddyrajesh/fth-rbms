{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfXwJx5LXSHC",
    "outputId": "4302d48d-f118-4dd1-af58-65f466c689f8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-63f4989ad8a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3Yu6MxcYC0X",
    "outputId": "d6c0026d-ced7-4363-9d8d-ad0abdf3d930"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECnz1RSIXSHI",
    "outputId": "06a7f715-a607-4468-81fa-04ff21f24b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                   1                             2\n",
      "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4  5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    }
   ],
   "source": [
    "movies_df = pd.read_csv('/content/drive/MyDrive/Projects/RBM-main/RBM_Recommendation/ml-1m/movies.dat', sep='::', header=None, engine='python',encoding='latin-1')\n",
    "print(movies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pmarUWdXSHM",
    "outputId": "7fa9c6a2-9e93-4f24-cd9f-17fdde48701f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1  2          3\n",
      "0  1  1193  5  978300760\n",
      "1  1   661  3  978302109\n",
      "2  1   914  3  978301968\n",
      "3  1  3408  4  978300275\n",
      "4  1  2355  5  978824291\n"
     ]
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('/content/drive/MyDrive/Projects/RBM-main/RBM_Recommendation/ml-1m/ratings.dat', sep='::', header=None, engine='python',encoding='latin-1')\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrJM0LA3XSHM",
    "outputId": "4230984d-4d6d-42ef-dc85-61dbed294823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID                               Title                        Genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n"
     ]
    }
   ],
   "source": [
    "movies_df.columns = ['MovieID', 'Title', 'Genres']\n",
    "ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "\n",
    "# Verify the changes done to the dataframes\n",
    "print(movies_df.head())\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qARfjTJXSHN",
    "outputId": "41a08b7e-f923-4c9d-cfb1-d21a864d8ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users:  6040\n",
      "Number of unique movies:  3706\n",
      "Number of total ratings:  1000209\n",
      "Average number of ratings per user:  165.5975165562914\n"
     ]
    }
   ],
   "source": [
    "n_users = ratings_df.UserID.unique().shape[0]\n",
    "n_movies = ratings_df.MovieID.unique().shape[0]\n",
    "n_ratings = len(ratings_df)\n",
    "\n",
    "avg_ratings_per_user = n_ratings/n_users\n",
    "\n",
    "print('Number of unique users: ', n_users)\n",
    "print('Number of unique movies: ', n_movies)\n",
    "print('Number of total ratings: ', n_ratings)\n",
    "print('Average number of ratings per user: ', avg_ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTyMNdn1XSHN",
    "outputId": "99faa04f-475a-46f6-823c-3f6982f68a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Movies in Dataset 3883\n",
      "   MovieID  ... List Index\n",
      "0        1  ...          0\n",
      "1        2  ...          1\n",
      "2        3  ...          2\n",
      "3        4  ...          3\n",
      "4        5  ...          4\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "   MovieID  List Index  UserID  Rating\n",
      "0        1           0       1       5\n",
      "1        1           0       6       4\n",
      "2        1           0       8       4\n",
      "3        1           0       9       5\n",
      "4        1           0      10       5\n"
     ]
    }
   ],
   "source": [
    "# Data Correction and Formatting\n",
    "print('The Number of Movies in Dataset', len(movies_df))\n",
    "\n",
    "\"\"\"\n",
    "- Our Movie ID's vary from 1 to 3952 while we have 3883 movies. \n",
    "- Due to this, we won't be able to index movies through their ID since we would get memory indexing errors. \n",
    "- To amend we can create a column that shows the spot in our list that particular movie is in:\n",
    "\"\"\"\n",
    "\n",
    "movies_df['List Index'] = movies_df.index\n",
    "print(movies_df.head())\n",
    "\n",
    "# Merge movies_df with ratings_df by MovieID\n",
    "merged_df = movies_df.merge(ratings_df, on='MovieID')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df = merged_df.drop('Timestamp', axis=1).drop('Title', axis=1).drop('Genres', axis=1)\n",
    "\n",
    "# Display the result\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVvwllTVXSHO",
    "outputId": "a458f1ff-b869-4a68-9944-cc789008e0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MovieID  List Index  UserID  Rating\n",
      "0             1           0       1       5\n",
      "1             1           0       6       4\n",
      "2             1           0       8       4\n",
      "3             1           0       9       5\n",
      "4             1           0      10       5\n",
      "...         ...         ...     ...     ...\n",
      "874890     3285        3216    1270       1\n",
      "883271     3357        3288    4896       5\n",
      "903916     3448        3379    4025       5\n",
      "928813     3548        3479     986       3\n",
      "933205     3568        3499    1226       3\n",
      "\n",
      "[30200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "user_Group = merged_df.groupby('UserID')\n",
    "print(user_Group.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAhwtwX1XSHO",
    "outputId": "35e62371-08c5-40ef-8234-675735c0ada7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5412 2683]\n",
      " [5440  904]\n",
      " [ 368 3717]\n",
      " ...\n",
      " [ 854 3102]\n",
      " [4033 3479]\n",
      " [ 786 1391]]\n",
      "[1.  0.8 0.8 ... 0.8 0.2 0.8]\n",
      "Size of train set:  800167\n",
      "Size of test set:  200042\n",
      "1.666654109954834\n"
     ]
    }
   ],
   "source": [
    "# Amount of users used for training\n",
    "start_time=time.time()\n",
    "\n",
    "ratings_df = ratings_df.sample(frac=1, random_state=42)\n",
    "x = ratings_df[[\"UserID\", \"MovieID\"]].values\n",
    "print(x)\n",
    "# Normalize the targets between 0 and 1. Makes it easy to train.\n",
    "y = user_Group[\"Rating\"].apply(lambda x:x/5.0).values\n",
    "print(y)\n",
    "# Assuming training on 80% of the data and validating on 10%.\n",
    "train_indices = int(0.8 * ratings_df.shape[0])\n",
    "X_train, X_val, Y_train, Y_val = (\n",
    "    x[:train_indices],\n",
    "    x[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")\n",
    "\n",
    "print('Size of train set: ', len(X_train))\n",
    "print('Size of test set: ', len(X_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(trX)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "exw3PFxoXSHO",
    "outputId": "164a92a7-a36c-4573-924f-c231656aae1b"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation MatMul: {{node MatMul}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[MatMul]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ffd7b220bc3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprv_hb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhiddenUnits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Train RBM with 15 Epochs, with Each Epoch using 10 batches with size 100, After training print out the error by epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1369\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1392\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1394\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation MatMul: node MatMul (defined at <ipython-input-11-ffd7b220bc3c>:12)  was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[MatMul]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node MatMul:\n Placeholder_3 (defined at <ipython-input-11-ffd7b220bc3c>:11)\t\n Placeholder_2 (defined at <ipython-input-11-ffd7b220bc3c>:8)"
     ]
    }
   ],
   "source": [
    "# Setting the models Parameters\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    start_time=time.time()\n",
    "    hiddenUnits = 50\n",
    "    visibleUnits = len(movies_df)\n",
    "    vb = tf.placeholder(tf.float32, [visibleUnits])  # Number of unique movies\n",
    "    hb = tf.placeholder(tf.float32, [hiddenUnits])  # Number of features were going to learn\n",
    "    W = tf.placeholder(tf.float32, [visibleUnits, hiddenUnits])  # Weight Matrix\n",
    "\n",
    "# Phase 1: Input Processing\n",
    "    v0 = tf.placeholder(\"float\", [None, visibleUnits])\n",
    "    _h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)  # Visible layer activation\n",
    "    h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))  # Gibb's Sampling\n",
    "\n",
    "# Phase 2: Reconstruction\n",
    "    _v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb)  # Hidden layer activation\n",
    "    v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))\n",
    "    h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)\n",
    "\n",
    "\"\"\" Set RBM Training Parameters \"\"\"\n",
    "\n",
    "# Learning rate\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    alpha = 1.0\n",
    "\n",
    "# Create the gradients\n",
    "    w_pos_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "    w_neg_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "\n",
    "# Calculate the Contrastive Divergence to maximize\n",
    "    CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "\n",
    "# Create methods to update the weights and biases\n",
    "    update_w = W + alpha * CD\n",
    "    update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)\n",
    "    update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)\n",
    "\n",
    "# Set the error function, here we use Mean Absolute Error Function\n",
    "    err = v0 - v1\n",
    "    err_sum = tf.reduce_mean(err*err)\n",
    "\n",
    "\"\"\" Initialize our Variables with Zeroes using Numpy Library \"\"\"\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "# Current weight\n",
    "    cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n",
    "\n",
    "# Current visible unit biases\n",
    "    cur_vb = np.zeros([visibleUnits], np.float32)\n",
    "\n",
    "# Current hidden unit biases\n",
    "    cur_hb = np.zeros([hiddenUnits], np.float32)\n",
    "\n",
    "# Previous weight\n",
    "    prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n",
    "\n",
    "# Previous visible unit biases\n",
    "    prv_vb = np.zeros([visibleUnits], np.float32)\n",
    "\n",
    "# Previous hidden unit biases\n",
    "    prv_hb = np.zeros([hiddenUnits], np.float32)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train RBM with 15 Epochs, with Each Epoch using 10 batches with size 100, After training print out the error by epoch\n",
    "    with tf.device(\"/device:GPU:0\"):    \n",
    "        epochs = 100\n",
    "        batchsize = 300\n",
    "        errors = []\n",
    "        for i in range(epochs):\n",
    "            for start, end in zip(range(0, len(X_train), batchsize), range(batchsize, len(X_train), batchsize)):\n",
    "                batch = X_train[start:end]\n",
    "                cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "                cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "                cur_hb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "                prv_w = cur_w\n",
    "                prv_vb = cur_vb\n",
    "                prv_hb = cur_hb\n",
    "            errors.append(sess.run(err_sum, feed_dict={v0: X_train, W: cur_w, vb: cur_vb, hb: cur_hb}))\n",
    "            print(errors[-1])\n",
    "    \n",
    "        print(time.time()-start_time)\n",
    "        plt.plot(errors)\n",
    "        plt.ylabel('Error')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "MVEVdyqgXSHP",
    "outputId": "7a73c908-d503-4da5-fdcb-19d074703cee",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1c4dc5649cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Select the input User\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m202\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0minputUser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Feeding in the User and Reconstructing the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recommendation System :-\n",
    "- We can now predict movies that an arbitrarily selected user might like. \n",
    "- This can be accomplished by feeding in the user's watched movie preferences into the RBM and then reconstructing the \n",
    "  input. \n",
    "- The values that the RBM gives us will attempt to estimate the user's preferences for movies that he hasn't watched \n",
    "  based on the preferences of the users that the RBM was trained on.\n",
    "\"\"\"\n",
    "\n",
    "# Select the input User\n",
    "user=202\n",
    "inputUser = [X_test[user]]\n",
    "\n",
    "# Feeding in the User and Reconstructing the input\n",
    "hh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n",
    "vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "feed = sess.run(hh0, feed_dict={v0: inputUser, W: prv_w, hb: prv_hb})\n",
    "rec = sess.run(vv1, feed_dict={hh0: feed, W: prv_w, vb: prv_vb})\n",
    "\n",
    "# List the 20 most recommended movies for our mock user by sorting it by their scores given by our model.\n",
    "scored_movies_df_input = movies_df\n",
    "scored_movies_df_input[\"Recommendation Score\"] = rec[0]\n",
    "print(scored_movies_df_input.sort_values([\"Recommendation Score\"], ascending=False).head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CuPoENmXSHR"
   },
   "outputs": [],
   "source": [
    "\"\"\" Recommend User what movies he has not watched yet \"\"\"\n",
    "\n",
    "# Find the mock user's UserID from the data\n",
    "mock_userid= merged_df.iloc[user]\n",
    "print(mock_userid)\n",
    "\n",
    "# Find all movies the mock user has watched before\n",
    "movies_df_input = merged_df[merged_df['UserID'] == (mock_userid.UserID)]\n",
    "print(movies_df_input.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c12TEUc2XSHR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Merge all movies that our mock users has watched with predicted scores based on his historical data: \"\"\"\n",
    "\n",
    "# Merging movies_df with ratings_df by MovieID\n",
    "merged_df_input = scored_movies_df_input.merge(movies_df_input, on='MovieID', how='outer')\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "merged_df_input = merged_df_input.drop('List Index_y', axis=1).drop('UserID', axis=1)\n",
    "\n",
    "# Sort and take a look at first 20 rows\n",
    "print(merged_df_input.sort_values(['Recommendation Score'], ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiiYDzlbXSHR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('results.csv', \"w+\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCDpyIBEXSHS"
   },
   "outputs": [],
   "source": [
    "sorted_result = merged_df_input.sort_values(by='Recommendation Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AozrQLCGXSHS"
   },
   "outputs": [],
   "source": [
    "sorted_result.to_csv(r'D:\\ACADEMICS\\RECOMMENDATION ENGINE\\results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyjX9Ot8XSHS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RBM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
